</p><div class="ptx" lang="en-US">In 1948 Claude E. Shannon in "The Mathematical Theory of Communication"' has introduced his famous formula for the entropy of a discrete set of probabilities p1, ... , p<sub>n</sub>: 
<br><center><pre><font size=5>H=-&sum;p<sub>i</sub>log<sub>2</sub>p<sub>i</sub></font></pre></center>
<br>We will apply this formula to an arbitrary text string by letting p<sub>i</sub> be the relative frequencies of occurrence of characters in the string. For example, the entropy of the string "Northeastern European Regional Contest" with the length of 38 characters (including 3 spaces) is 3.883 with 3 digits after decimal point. The following table shows relative frequencies and the corresponding summands for the entropy of this string.
<br><center><img src=images/1898_1.jpg></center>
<br>Your task is to find a string with the given entropy.  </div><p class="pst">Input</p><div class="ptx" lang="en-US">The input consists of a single real number H (0.00 <= H <= 6.00) with 2 digits after decimal point. </div><p class="pst">Output</p><div class="ptx" lang="en-US">Write to the output file a line with a single string of at least one and up to 1000 characters '0'-'9', 'a'-'z', 'A'-'Z', '.' (dot), and spaces. This string must have the entropy within 0.005 of H. </div><p class="pst">Sample Input</p><pre class="sio">3.88
</pre><p class="pst">Sample Output</p><pre class="sio">Northeastern European Regional Contest
</pre><p class="pst">